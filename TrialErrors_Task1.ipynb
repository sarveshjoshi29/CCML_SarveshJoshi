{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69c29420-3857-4faf-a1a4-78e562c61f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import prince\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cbfeb9-236b-45bc-8f03-53d25b69283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data = pd.read_csv(\"Dataset.csv\")\n",
    "student_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc09b7-1f6f-435e-84fc-226c4b5810d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3d3f4-788b-4439-b149-1bce79a513f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1f4b5-68be-4ce1-b0f6-1d8452c82d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data.fillna({\"Fedu\" : student_data[\"Fedu\"].median() , \"higher\" : \"yes\",\"famsize\" : \"GT3\" , \"traveltime\" : student_data[\"traveltime\"].median() , \"freetime\" : student_data[\"freetime\"].median(), \"absences\" : student_data[\"absences\"].median()},inplace = True)\n",
    "#print(student_data[\"absences\"].median())\n",
    "#print(student_data[student_data[\"romantic\"]==\"yes\"].count())\n",
    "#student_data.head(30)\n",
    "student_data.drop([\"G2\"],axis=1,inplace = True)\n",
    "student_data.dropna(axis = 0,inplace = True)\n",
    "student_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018a37a-f2e3-40b7-b2f5-cb1555086c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "romantic_true = student_data[student_data[\"romantic\"] == \"yes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4690a6a0-509b-4965-9cb8-2bdd9107469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = romantic_true,x = \"Feature_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f538d9-ec7a-4058-ba8a-0d4dfdac4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "romantic_false = student_data[student_data[\"romantic\"] == \"no\"]\n",
    "sns.histplot(data = romantic_false, x = \"Feature_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa58d88-37fe-422c-a2c5-f27e2b8453d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mappings = {}\n",
    "for col in student_data.columns:\n",
    "    if student_data[col].dtype == \"object\":\n",
    "        student_data[col] = student_data[col].astype(\"category\")\n",
    "        student_data[col+\"_code\"] = student_data[col].astype(\"category\").cat.codes\n",
    "        category_mappings[col] = dict(enumerate(student_data[col].cat.categories))\n",
    "corr_matrix = student_data.corr(numeric_only = True)\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-notebook\")\n",
    "plt.figure(figsize=(20, 10)) \n",
    "sns.heatmap(corr_matrix,annot = True , cmap = \"coolwarm\",fmt=\".2f\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b808fb7d-a517-40b6-9236-2931c40e6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature_3 might represent frequency of off-campus visits because high correlation with goout and Dalc etc.\n",
    "# Feature_2 has +ve impact on grades if grades are above a certain threshold\n",
    "# Feature_3 varies inversely to feature_2... so feature 2 could be attendance etc.\n",
    "#\"Feature_1 varies inversely with schoolsup and grades and inversely with freetime. It could represent academic stress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f74fc-b039-4869-b2aa-60407a62ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c5119-0175-4a72-8a2b-0e86d385406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193bb81c-8f8f-420e-8d7c-2fbb15f1a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data = student_data ,x=\"G3\",y=\"Feature_2\",color = \"Red\",errorbar = None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0e46e-2e3c-4226-81f8-79b16f80c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = student_data ,x=\"freetime\",y=\"Feature_2\",color = \"Lawngreen\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259879a9-4dc0-4648-9be2-5c1888425368",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = student_data ,x=\"freetime\",y=\"Feature_1\",color = \"Magenta\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b1273-9799-4950-9b22-66617a3d1795",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = student_data ,x=\"schoolsup\",y=\"Feature_1\",color = \"Yellow\");\n",
    "#print(student_data[\"schoolsup\"].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2842c-7022-4943-bd9c-ff742460c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = student_data ,x=\"G1\",y=\"Feature_1\",color = \"violet\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94382f91-4c81-473c-9a27-5036eacd783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How are failures and grades correlated?\n",
    "sns.barplot(data = student_data[student_data[\"G3\"]!=5] ,x=\"G3\",y=\"failures\",color = \"blue\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda35dd2-a6ce-4d84-9e9c-aee667a00b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students of which school have better grades? This gives an insight into quality of education in both schools\n",
    "sns.boxplot(data = student_data, x=\"school\",y = \"G3\",color = \"orange\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee12ae1-854d-4854-99a5-95dbec428ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is travel time related to location of resdience of students? This gives us an idea of whether the school is located in a rural area or urban area\n",
    "#Conclusion is that both schools are located in an urban area\n",
    "sns.violinplot(data = student_data[student_data[\"school_code\"] == 0], x=\"address\" , y = \"traveltime\",color = \"red\")\n",
    "plt.show()\n",
    "sns.violinplot(data = student_data[student_data[\"school_code\"] == 1], x=\"address\" , y = \"traveltime\",color = \"#a1c9f4\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b80ce9-0729-4241-9def-c854275717bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is there a correlation between grades and the interest in pursuing higher studies?\n",
    "sns.boxplot(data = student_data, x=\"higher\",y = \"G3\",color = \"purple\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1db6d3-b31e-4a74-aa9e-4d5a0f1fe4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Internet vs school\n",
    "mosaic(student_data,[\"school\",\"internet\"]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154babb5-4295-43b4-99c9-d05b782a089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mjob vs Medu\n",
    "sns.lineplot(data = student_data, x=\"Mjob\",y = \"Medu\",color = \"pink\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c971c-12d2-4b00-a0b6-16e85e731fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fjob vs Fedu\n",
    "sns.lineplot(data = student_data, x=\"Fjob\",y = \"Fedu\",color = \"red\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b6f61-1a94-4570-adef-e9694bbf08fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MANUAL TRY TO PERFORM MULTIVARIABLE GRADIENT DESCENT BUT OBVIOUSLY ITS TAKING A HELL LOT OF TIME :)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def costFunction(w,b,data):\n",
    "    cost = 0\n",
    "    for i in range(0,len(data)):\n",
    "        x = []\n",
    "        yi = data.iloc[i,len(data.columns)-1]\n",
    "        for j in range (0,len(data.columns)-1):\n",
    "            x.append(data.iloc[i,j])\n",
    "        x = np.array(x)\n",
    "        #print(\"x is \" ,end = \"\")\n",
    "        #print(x)\n",
    "        temp = np.dot(w,x) + b\n",
    "        fxi = sigmoid(temp)\n",
    "        cost = cost + yi*(np.log(fxi)) + (1-yi)*(np.log(1-fxi))\n",
    "    cost = cost*(-1)/len(data)\n",
    "    \n",
    "    return cost\n",
    "def gradient(w,b,data):\n",
    "    grad = np.zeros(len(data.columns)-1)\n",
    "    gradb = 0\n",
    "    for i in range(0,len(data)):\n",
    "        x = []\n",
    "        yi = data.iloc[i,len(data.columns)-1]\n",
    "        for j in range (0,len(data.columns)-1):\n",
    "            x.append(data.iloc[i,j])\n",
    "        x = np.array(x)\n",
    "        #print(\"x is \" ,end = \"\")\n",
    "        #print(x)\n",
    "        temp = np.dot(w,x) + b\n",
    "        fxi = sigmoid(temp)\n",
    "        factor = fxi-yi\n",
    "        gradb = gradb + factor\n",
    "        grad = grad + factor*x\n",
    "    grad = grad/len(data)\n",
    "    gradb = gradb/len(data)\n",
    "    ans = [grad,gradb]\n",
    "    return ans\n",
    "\n",
    "def gradientDescent(w_init,b_init,data):\n",
    "    gradW,gradB = gradient(w_init,b_init,data)\n",
    "    alpha = 0.5\n",
    "    while(np.linalg.norm(gradW)>0.001 and abs(gradB) >0.001):\n",
    "        gradW,gradB = gradient(w_init,b_init,data)\n",
    "        w_init = w_init - alpha*gradW\n",
    "        b_init = b_init - alpha*gradB\n",
    "        temp = [w_init,b_init]\n",
    "        #print(temp)\n",
    "    ans = [w_init,b_init]\n",
    "\n",
    "    return ans\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e923391-06e1-4ac2-8711-826b5a2840ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# I DONT KNOW WHAT MCA IS BUT IT SEEMS TO BE LIKE PCA FOR CATGORICAL FIELDS. TRYING IT OUT:-\n",
    "\n",
    "mca = prince.MCA(n_components=2)\n",
    "student_dataLR = mca.fit_transform(student_data.drop(\"romantic_code\",axis=1))\n",
    "#student_dataLR =  PCA(n_components=2).fit_transform(student_data,numeric_only = True)\n",
    "student_dataLR.reset_index(inplace= True,drop = True)\n",
    "student_data.reset_index(inplace= True,drop = True)\n",
    "student_dataLR[\"romantic_code\"] = student_data[\"romantic_code\"]\n",
    "student_dataLR\n",
    "#print(student_dataLR.iloc[0,len(student_dataLR.columns)-1])\n",
    "# w = np.ones(len(student_dataLR)-1)\n",
    "# b = 0\n",
    "#print(gradient(w,b,student_dataLR))\n",
    "# w,b = gradientDescent(w,b,student_dataLR)\n",
    "# print(w)\n",
    "# print(b)\n",
    "      \n",
    "      \n",
    "#student_dataLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735df74e-7608-4967-abe6-49dc9f16c006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eccb68-1456-4b5c-a816-d22094d628dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = student_dataLR.drop(\"romantic_code\",axis =1)\n",
    "y_df = student_dataLR[\"romantic_code\"]\n",
    "y_arr = y_df.to_numpy()\n",
    "x_arr = x_df.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_arr, y_arr, test_size=0.2)\n",
    "modelLR = LogisticRegression()\n",
    "modelDT = DecisionTreeClassifier()\n",
    "modelLR.fit(X_train, y_train)\n",
    "y_predLR = modelLR.predict(X_test)\n",
    "print(\"Accuracy of Logistic Regression :\", accuracy_score(y_test, y_predLR))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_predLR))\n",
    "\n",
    "modelDT.fit(X_train, y_train)\n",
    "y_predDT = modelDT.predict(X_test)\n",
    "print(\"Accuracy of Decision Tree:\", accuracy_score(y_test, y_predDT))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_predDT))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Accuracy of KNN :\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Accuracy of RandForest\", rf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af31a08-d0ba-45a7-9440-7291a40158f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature scaling (optional)\n",
    "#SelectKbest (k=2)\n",
    "# models used -- LR , DT, RF, XGB\n",
    "\n",
    "\n",
    "# TO DO -- UNDERSTAND FEATURE SCALING ,\n",
    "#sELECTkBEST ,\n",
    "\n",
    "#GRIDSEARCH , \n",
    "\n",
    "#PCA implementation ---- later \n",
    "# HOW TO PLOT DECISION BOUNDARY\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# mca = prince.MCA(n_components=2)\n",
    "# student_dataLR = mca.fit_transform(student_data.drop(\"romantic_code\",axis=1))\n",
    "#student_dataLR =  PCA(n_components=2).fit_transform(student_data,numeric_only = True)\n",
    "\n",
    "#student_data.dtypes\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# numeric_student_data = scaler.fit_transform(numeric_student_data)\n",
    "#pca = PCA(n_components = 2)\n",
    "\n",
    "X = student_data.drop([\"romantic_code\"],axis=1)\n",
    "#X = student_data.drop([\"romantic_code\",\"sex_code\",\"school_code\",\"reason_code\"],axis = 1)\n",
    "y = student_data[\"romantic_code\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numeric_X = X.select_dtypes(include = \"number\")\n",
    "selector = SelectKBest( k=10) #k = 10 is working really great\n",
    "numeric_Xarr = selector.fit_transform(numeric_X,y)\n",
    "\n",
    "\n",
    "selected_features = numeric_X.columns[selector.get_support()]\n",
    "print(\"Selected features:\", selected_features.tolist())\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numeric_Xarr = scaler.fit_transform(numeric_Xarr)\n",
    "\n",
    "#numeric_X = numeric_X[[\"goout\",\"Feature_3\"]]\n",
    "#print(numeric_X.dtypes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(numeric_Xarr, y, test_size=0.2,random_state =69) #randomstate = 69 works reall great\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(X_train)\n",
    "# pca = PCA(n_components = 2)\n",
    "# X_train = pca.fit_transform(X_train)\n",
    "# X_test = pca.fit_transform(X_test)\n",
    "# X = pca.fit_transform(numeric_X)\n",
    "#print(X_train)\n",
    "modelLR = LogisticRegression()\n",
    "modelDT = DecisionTreeClassifier()\n",
    "modelKNN = KNeighborsClassifier(n_neighbors=7)\n",
    "modelRF = RandomForestClassifier()\n",
    "modelXGB = xgb.XGBClassifier()\n",
    "modelBayes = GaussianNB()\n",
    "\n",
    "# modelLR.fit(X_train, y_train)\n",
    "# y_predLR = modelLR.predict(X_test)\n",
    "# print(\"Accuracy of Logistic Regression :\", accuracy_score(y_test, y_predLR))\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_predLR))\n",
    "\n",
    "paramRF = {\n",
    "    \"n_estimators\" : [25,30,35],\n",
    "    \"criterion\" : [\"gini\",\"entropy\",\"log_loss\"],\n",
    "     \"min_samples_split\" : [0.05,0.06],\n",
    "     \"min_samples_leaf\" : [5,10,15] ,\n",
    "     \"max_depth\" : [50,80]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator = modelRF , param_grid = paramRF,cv = 6)\n",
    "grid.fit(numeric_Xarr,y)\n",
    "# Best model and parameters\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best CV score:\", 100*grid.best_score_)\n",
    "#print(modelRF.get_params())\n",
    "# modelDT.fit(X_train, y_train)\n",
    "# y_predDT = modelDT.predict(X_test)\n",
    "# print(\"Accuracy of Decision Tree:\", accuracy_score(y_test, y_predDT))\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_predDT))\n",
    "\n",
    "\n",
    "# modelKNN.fit(X_train, y_train)\n",
    "# y_pred = modelKNN.predict(X_test)\n",
    "# print(\"Accuracy of KNN :\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# modelBayes.fit(X_train, y_train)\n",
    "# print(\"Accuracy of RandForest\", modelBayes.score(X_test, y_test))\n",
    "\n",
    "# print(modelBayes.get_params())\n",
    "# modelXGB.fit(X_train,y_train)\n",
    "# print(\"Accuracy of XGB\", modelXGB.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# svc = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)\n",
    "# svc.fit(X_train, y_train)\n",
    "\n",
    "# # Predict\n",
    "# y_pred = svc.predict(X_test)\n",
    "\n",
    "# # Evaluate\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc7146-2979-4f69-998e-f98e40ea0ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(numeric_Xarr, y, test_size=0.2,random_state = 69) #randomstate = 69 works reall great\n",
    "\n",
    "modelLR = LogisticRegression()\n",
    "modelDT = DecisionTreeClassifier()\n",
    "modelKNN = KNeighborsClassifier(n_neighbors=7)\n",
    "modelRF = RandomForestClassifier()\n",
    "modelXGB = xgb.XGBClassifier()\n",
    "\n",
    "paramRF = {\n",
    "    \"n_estimators\" : [20,40],\n",
    "    \"criterion\" : [\"gini\",\"entropy\",\"log_loss\"],\n",
    "     \"min_samples_split\" : [0.01,0.05,0.08],\n",
    "     \"min_samples_leaf\" : [2,5,10] ,\n",
    "     \"max_depth\" : [50,80]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator = modelRF , param_grid = paramRF,cv = 4)\n",
    "\n",
    "\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Accuracy of RandForest\", grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c933f3a-a1aa-4385-b174-e4db662d9089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
